{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e318f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import calendar\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from datetime import datetime\n",
    "from typing_extensions import Literal\n",
    "import spacy\n",
    "from pii_codex.services.analysis_service import PIIAnalysisService\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk import pos_tag\n",
    "import warnings\n",
    "from openpyxl import load_workbook\n",
    "import logging\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pii_analysis_service = PIIAnalysisService() # Install PII_Codex package following instrucions on Github: https://github.com/EdyVision/pii-codex?tab=readme-ov-file\n",
    "\n",
    "# Load SpaCy's English NER model\n",
    "nlp=spacy.load(\"en_core_web_sm\")  # Spacy's trained pipeline package, can be downloaded python -m spacy download en_core_web_sm\n",
    "\n",
    "# Set up logging configuration to suppress NLTK download messages\n",
    "logging.getLogger('nltk').setLevel(logging.ERROR)\n",
    "\n",
    "def download_missing_packages():\n",
    "    # List of required packages and their identifiers\n",
    "    packages = {\n",
    "        'averaged_perceptron_tagger': 'taggers/averaged_perceptron_tagger',\n",
    "        'maxent_ne_chunker': 'chunkers/maxent_ne_chunker',\n",
    "        'words': 'corpora/words',\n",
    "        'punkt': 'tokenizers/punkt'\n",
    "    }\n",
    "    \n",
    "    for package, identifier in packages.items():\n",
    "        try:\n",
    "            nltk.data.find(identifier)\n",
    "            print(f\"Package '{package}' is already installed.\")\n",
    "        except LookupError:\n",
    "            print(f\"Package '{package}' not found. Downloading...\")\n",
    "            nltk.download(package)\n",
    "            print(f\"Package '{package}' has been downloaded and installed.\")\n",
    "\n",
    "# Run the function to check and download missing packages\n",
    "download_missing_packages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "796d9acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_Scrubbed= pd.read_csv(\"Major Safety Events_Scrubbed.csv\") # load data\n",
    "df= pd.read_excel(\"SS Major Events 2014 to present - Dossier - 2024-08-01T135318.165.xlsx\") # load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e5b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= df.copy()\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1613cbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 1.3433198928833008 seconds\n"
     ]
    }
   ],
   "source": [
    "def scrub_vehicle_numbers(text):\n",
    "    def redact_numbers(match):\n",
    "        # Check if the match starts with a vehicle type (like \"Coach\") followed by numbers\n",
    "        if re.match(r'\\b((?:Bus|Train|Coach|Car|Vehicle|MV|VIN|Cars|Lead|LRV)\\s*)\\d+', match.group(), re.IGNORECASE):\n",
    "            # Redact only the numbers, preserving the vehicle type\n",
    "            return match.group(1) + \"<REDACTED>\"\n",
    "        else:\n",
    "            return match.group()  # Return original match if not following a vehicle type\n",
    "    def redact_hyphen_numbers(match):\n",
    "        # Check if the match is numbers separated by hyphens and not in MM-DD-YYYY format\n",
    "        if re.match(r'\\b\\d+-\\d+\\b', match.group()) and not re.match(r'\\b\\d{1,2}-\\d{1,2}-\\d{4}\\b', match.group()):\n",
    "            return '<REDACTED>'\n",
    "        else:\n",
    "            return match.group()  # Return original match\n",
    "        \n",
    "        \n",
    "    patterns = [\n",
    "        (r'\\b((?:Bus|Train|Coach|Car|Vehicle|MV|VIN|Cars|Lead|cab)\\s*)\\d+\\b', redact_numbers),  # Redact numbers following vehicle types\n",
    "        (r'(?<=#)\\d+', \"<REDACTED>\"),        # Redact numbers following '#'\n",
    "        (r'#:\\s*(\\d+)', lambda match: '#: <REDACTED>'),\n",
    "        (r'#\\s*(?:\\(\\d+\\)|\\d+)', \"<REDACTED>\"), \n",
    "        (r'\\b\\d+@\\S+\\b', \"<REDACTED>\"),     # Redact patterns like \"03202022 @example\" \n",
    "        (r\"SITS\\s*#?\\s*\\d+\", \"<REDACTED>\"),  # Redact digits following SITS \n",
    "        (r'LRV\\s*#?\\s*\\d+(?:-\\d+)*', \"<REDACTED>\"),  # Redact LRV licence plate\n",
    "        (r'\\b\\d+-\\d+(?:-\\d+)*\\b', redact_hyphen_numbers),  # Redact numbers separated by hyphens if not in date format\n",
    "        (r'\\b(?:[A-Z]+(?:-[A-Z]*)*\\d+(?:-\\d+)*)\\b', \"<REDACTED>\" ),\n",
    "        (r'\\bUNIT\\s*#?\\d+|\\bUnit\\s*#?\\d+', \"<REDACTED>\"),\n",
    "        (r'\\btrain\\s+consist[^\\d]*(\\d+(?:\\s+\\d+)*)(?!\\d)', ' train consist <REDACTED>' ),\n",
    "        (r'\\([^\\)]*[0-9a-zA-Z][^\\)]*\\)',\"<REDACTED>\"),\n",
    "        (r\"(?i)\\bDOB\\s*[\\d\\/\\-\\.\\s]{7,}|\\bDOB[\\d\\/\\-\\.\\s]{7,}\",  \"DOB <REDACTED>\"), # Redact DOB\n",
    "        (r'(?i)license\\s*plate\\s*(?:\\(?\\s*([^\\)\\s]+)\\s*\\)?)', \"license plate <REDACTED>\"), # Redact license plate\n",
    "        (r'\\b\\d{3}\\b', \"\"),\n",
    "       \n",
    "        \n",
    "    ]\n",
    "    \n",
    "    # Apply redaction patterns to the entire text\n",
    "    for pattern, replacement in patterns:\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)   \n",
    "    \n",
    "    return text\n",
    "    \n",
    "    \n",
    "#Function to scrub PII after occupation roles\n",
    "def scrub_names_after_occupation_title(text):\n",
    "    # Define regular expression patterns\n",
    "    name_pattern_7= r'\\b[A-Z]\\.(?: [A-Z][a-zA-Z\\-]+)+\\b' #pattern to match names formatted as initials followed by one or more last names\n",
    "    name_pattern_1 = r'\\b(?:Conductor|Operator|police\\s+officer)\\s+(?!Assault(?:ed)?|Stated?)\\b[A-Z][a-zA-Z]*\\b(?:\\s+[A-Z][a-zA-Z]*)*' # Pattern to match titles like Conductor or Operator followed by a name\n",
    "    name_pattern_2 =  r'\\b(OPERATOR|CONDUCTOR|engineer|officer|superintendent (transportation|of transportation)|track (walker|foreman)|supervisor|chief officer (of operations|maintainance of way|superintendent (of (transportation|infrastructure|maintenance of way)|system safety|infrastructure)|deputy superintendent))\\b(?!\\s+(?:\\b(?:WAS|IS|HAS|HAD)\\b|\\w+(?:ED|ING)))\\s+([A-Z][a-zA-Z]*\\b(?:\\s+[A-Z][a-zA-Z]*)*)'\n",
    "    name_pattern_3=  r'Conductor\\s*\\((?!ASSAULT(?:ED|ING)?)[^)]*\\)' #Pattern to match Conductor followed by a name in parentheses with exceptions : Assault, Assaulted\n",
    "    name_pattern_4 =  r'Operator\\s*\\((?!ASSAULT(?:ED|ING)?)[^)]*\\)'#Pattern to match Conductor followed by a name in parentheses with exceptions : Assault, Assaulted\n",
    "    name_pattern_5 = r'\\b(?:Det|Nurse|Police\\s+Officer|P\\.O\\.|Sergeant|Sgt\\.|OP\\.|CON\\.|Ofc\\.|SGT|SGT\\.|OP|PO|Det\\.|police\\s+officer|Mgr\\.|Mgr|Mntr\\.)\\s+\\b[A-Z][a-zA-Z]*\\b(?:\\s+[A-Z][a-zA-Z]*)*'## Pattern to match various titles followed by a name (e.g., Det, Nurse, Sgt, Police Officer, P.O., Sergeant)\n",
    "    name_pattern_6 = r'(?:Dispatcher|Driver|Supervisor|Planner|Scheduler|Agent|Inspector|Officer|Captain|Deckhand|TM|CS|ATM|SME|Sgt|Detective|Capt\\.|Lt\\.|Sgt\\.|detective|cs|Engineer|superintendent|Superintendent)\\s+\\b[A-Z][A-Za-z-]*\\b(?:\\s+[A-Z][A-Za-z-]*)*' # # Pattern to match transit occupation title followed by a name\n",
    "    name_pattern_8= r'(?i)(PO\\.|CON\\.|Ofc\\.|Lt|Lt\\.|Sgt\\.)(\\s+[A-Z][a-zA-Z]*\\s+[A-Z][a-zA-Z]*)'# \n",
    "    name_pattern_9= r'by\\s[A-Z][A-Z\\-\\'\\s]+(?:,\\s[A-Z][A-Z\\-\\'\\s]+)*'\n",
    "        \n",
    "    \n",
    "    # Redact sensitive information\n",
    "    text = re.sub(name_pattern_7, ' <REDACTED> ', text)\n",
    "    text = re.sub(name_pattern_1, lambda match: match.group(0).split()[0] + ' <REDACTED>', text)  \n",
    "    text = re.sub(name_pattern_2, lambda match: match.group(1) + ' <REDACTED>', text )\n",
    "    text = re.sub(name_pattern_3, 'Conductor <REDACTED>', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(name_pattern_4, 'Operator <REDACTED>', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(name_pattern_5, lambda match: match.group(0).split()[0] + ' <REDACTED>', text)\n",
    "    text = re.sub(name_pattern_6, lambda match: match.group(0).split()[0] + ' <REDACTED>', text)\n",
    "    text = re.sub(name_pattern_8, r'\\1 <REDACTED> <REDACTED>', text)\n",
    "    text = re.sub(name_pattern_9, \"[REDACTED]\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Function to handle PII Codex exceptions \n",
    "def handle_pii_exception(text):\n",
    "    doc = nlp(text)\n",
    "    scrubbed_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.ent_type_ in [\"PERSON\", \"LOC\" ]:  #\"GPE\", \"ORG\", \"MONEY\", \"QUANTITY\", \"EVENT\", \"FAC\", \"NORP\"\n",
    "            scrubbed_tokens.append(\"<REDACTED>\")\n",
    "        else:\n",
    "            scrubbed_tokens.append(token.text)\n",
    "\n",
    "    scrubbed_text = ' '.join(scrubbed_tokens)\n",
    "    \n",
    "\n",
    "    \n",
    "    return scrubbed_text\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Define a function to normalize a single line of text\n",
    "    def normalize_line(line):\n",
    "        tokens = nltk.word_tokenize(line)\n",
    "        tagged = pos_tag(tokens)\n",
    "        named_entities = ne_chunk(tagged)\n",
    "\n",
    "        normalized_line = []\n",
    "        for subtree in named_entities:\n",
    "            if isinstance(subtree, nltk.Tree):  # Named entity\n",
    "                # Preserve case for named entities\n",
    "                normalized_line.append(' '.join(word for word, tag in subtree.leaves()))\n",
    "            else:\n",
    "                word, tag = subtree\n",
    "                if word.isupper() and not re.match(r'^[A-Z]\\.', word):  # Skip lowercase conversion for initials\n",
    "                    normalized_line.append(word.lower())\n",
    "                else:\n",
    "                    normalized_line.append(word)\n",
    "\n",
    "        normalized_line_str = ' '.join(normalized_line)\n",
    "        # Remove space before punctuation\n",
    "        normalized_line_str = re.sub(r'\\s+([,.!?])', r'\\1', normalized_line_str)\n",
    "        return normalized_line_str\n",
    "\n",
    "    # Split text into lines\n",
    "    lines = text.splitlines()\n",
    "    \n",
    "    # Normalize each line individually\n",
    "    normalized_lines = [normalize_line(line) for line in lines]\n",
    "    \n",
    "    # Join lines back with the original line breaks\n",
    "    return '\\n'.join(normalized_lines)\n",
    "\n",
    "def convert_to_lower_except_first(text):\n",
    "    patterns = ['DET', 'CAPT', 'OFC', 'SGT', 'MR', 'DR', 'SUPV', 'LT', 'TD', \"MGR\", \"PO\", \"SIS\", \"SLD\"]\n",
    "    pattern_regex = r'\\b(?:{})\\b'.format('|'.join(patterns))\n",
    "\n",
    "    def replace(match):\n",
    "        return match.group(0)[0] + match.group(0)[1:].lower()\n",
    "\n",
    "    result = re.sub(pattern_regex, replace, text)\n",
    "    return result\n",
    "\n",
    "def convert_specific_strings_to_lower(text):\n",
    "    # Define the strings to convert to lowercase\n",
    "    strings_to_convert = [\n",
    "        \"Victim\", \"OPERATOR\", \"CONDUCTOR\", \"ENGINEER\", \"OFFICER\", \"ASSAULT\",\n",
    "        \"SUPERINTENDENT TRANSPORTATION\", \"TRACK WALKER\", \"SUPERVISOR\",\"VAN\",\n",
    "        \"SUPERINTENDENT OF TRANSPORTATION\", \"CHIEF OFFICER OF OPERATIONS\",\n",
    "        \"SUPERINTENDENT\", \"DEPUTY SUPERINTENDENT\",\"SYSTEM SAFETY\", \"CAPTAIN\",\n",
    "        \"SUPERINTENDENT OF INFRASTRUCTURE\", \"SUPERVISOR OF INFRASTRUCTURE\",\n",
    "        \"TRACK FOREMAN\", \"SUPERINTENDENT OF MAINTENANCE OF WAY\", \"CHIEF OFFICER MAINTAINANCE OF WAY\"\n",
    "    ]\n",
    "    \n",
    "    # Create a regex pattern to match the exact strings\n",
    "    pattern = re.compile(r'\\b(?:' + '|'.join(map(re.escape, strings_to_convert)) + r')\\b', re.IGNORECASE)\n",
    "    \n",
    "    # Convert matched strings to lowercase\n",
    "    def replace(match):\n",
    "        return match.group(0).lower()\n",
    "    \n",
    "    return pattern.sub(replace, text)\n",
    "\n",
    "\n",
    "# List of patterns to match\n",
    "patterns_lowercase_no_space = {\n",
    "    'ac', 'ad', 'af', 'ao', 'bb', 'bc', 'bd', 'bf', 'bh', 'bi', 'bm', 'bo', 'bq', 'bt', 'bw',\n",
    "    'ca', 'ce', 'cm', 'co', 'cr', 'cs', 'cv', 'cw', 'da', 'db', 'dn', 'dr', 'ds', 'ea', 'eb', 'ef',\n",
    "    'ej', 'el', 'eo', 'er', 'es', 'ew', 'fa', 'fb', 'fg', 'fh', 'fi', 'fm', 'fn', 'fo', 'fp', 'fr',\n",
    "    'fs', 'fu', 'fw', 'gs', 'ha', 'hf', 'hm', 'hr', 'hw', 'ib', 'io', 'jz', 'kl', 'kt',\n",
    "    'la', 'lc', 'lf', 'lr', 'ls', 'ma', 'mb', 'mc', 'mh', 'mn', 'mo', 'mr', 'ms', 'mt', 'mu', 'mv',\n",
    "    'mw', 'na', 'nb', 'nd', 'ne', 'nl', 'nn', 'nq', 'nr', 'ns', 'nw', 'ob', 'od', 'op', 'ov',\n",
    "    'pa', 'po', 'ps', 'pu', 'pv', 'qb', 'qn', 'qw', 'ra', 'rc', 're', 'rf', 'ro', 'rr', 'rs', 'rt',\n",
    "    'rw', 'sa', 'sb', 'sc', 'sd', 'se', 'sh', 'si', 'sm', 'sn', 'so', 'sp', 'ss', 'sv', 'sw', 'ta',\n",
    "    'tc', 'td', 'tf', 'tp', 'tr', 'ts', 'tw', 'uc', 'vc', 'vo', 'wa', 'wb', 'wc', 'wf', 'wh',\n",
    "    'wm', 'wo', 'ws', 'wt', 'ww', 'xo', 'ym', 'yo'\n",
    "}\n",
    "\n",
    "# Create a dictionary for reverting patterns to uppercase\n",
    "reverse_pattern_dict = {pattern: pattern.upper() for pattern in patterns_lowercase_no_space}\n",
    "\n",
    "# Create a regex pattern that matches any of the lowercase patterns\n",
    "patterns_regex = r'\\b(?:' + '|'.join(re.escape(pattern) for pattern in patterns_lowercase_no_space) + r')\\b'\n",
    "\n",
    "def revert_patterns(text):\n",
    "    \"\"\"\n",
    "    Reverts matched patterns in the text to their uppercase form.\n",
    "\n",
    "    Parameters:\n",
    "    - text: The input text containing patterns to be reverted.\n",
    "\n",
    "    Returns:\n",
    "    - The text with patterns reverted to uppercase.\n",
    "    \"\"\"\n",
    "    def replace_match(match):\n",
    "        matched_pattern = match.group(0)\n",
    "        return reverse_pattern_dict.get(matched_pattern, matched_pattern)\n",
    "    \n",
    "    # Use re.sub with a function to replace matches\n",
    "    return re.sub(patterns_regex, replace_match, text)\n",
    "\n",
    "# Function to process and scrub text\n",
    "def process_and_scrub_text(text):\n",
    "    \n",
    "    text = text.replace(\"  \", \" \")  # Replace double spaces with single space\n",
    "    text = text.replace('#', '')\n",
    "    text = re.sub(r'\\b([a-zA-Z])\\/([a-zA-Z])\\b', lambda m: (m.group(1) + m.group(2)).lower(), text)\n",
    "    # Transform specific patterns to lowercase except for the first letter\n",
    "    text = convert_to_lower_except_first(text)\n",
    "    text = re.sub(r'\\(([^)]+)\\)', lambda m: f\"({ ' '.join(word.capitalize() if len(word) > 2 else word for word in m.group(1).split()) })\", text)\n",
    "    text = re.sub(r'\\bDet\\b(?!\\.)', 'detective', text)\n",
    "    text = text.replace('P.O.', 'Officer').replace(\"Sgt.\", \"Sgt\").replace(\"Supv.\", \"supervisor\")\n",
    "    text= convert_specific_strings_to_lower(text)\n",
    "    text = re.sub('(?<=[a-z])(?=[A-Z])', ' ', text) #\n",
    "    text = normalize_text(text)\n",
    "    text = scrub_vehicle_numbers(text) # Call function to scrub vehicle numbers\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    \n",
    "   \n",
    "    try: # Call PII to scrub sensitive info within the text column\n",
    "        analysis_results = pii_analysis_service.analyze_collection([text])\n",
    "        results = analysis_results.to_dict()\n",
    "        sanitized_texts = [result[\"sanitized_text\"] for result in results[\"analyses\"]]\n",
    "        scrubbed_text_ = sanitized_texts[0]\n",
    "        scrubbed_text_= scrub_names_after_occupation_title(sanitized_texts[0])\n",
    "        scrubbed_text_= handle_pii_exception(scrubbed_text_) \n",
    "        \n",
    "        \n",
    "    except Exception as e:  # This will handle PII exceptions if any and print an error message \n",
    "        print(f\"An error occurred while processing the text: {text}. Error: {e}\")\n",
    "        scrubbed_text_ = handle_pii_exception(text)\n",
    "     \n",
    "    \n",
    "    \n",
    "    return scrubbed_text_  \n",
    "\n",
    "start = time.time()  # Start time for overall process\n",
    "\n",
    "\n",
    "# Create an empty list to store the scrubbed texts\n",
    "scrubbed_texts = []\n",
    "\n",
    "with tqdm(total=len(df2)) as pbar:\n",
    "    for _, row in df2.iterrows():\n",
    "        scrubbed_text = process_and_scrub_text(row[ 'Event Description'])\n",
    "        scrubbed_texts.append(scrubbed_text)\n",
    "        pbar.update(1)  # Update progress bar\n",
    "\n",
    "end = time.time()  # End time for overall process\n",
    "print(f\"Total processing time: {end - start} seconds\")\n",
    "\n",
    "\n",
    "# Replace the 'Incident_Description' column with the scrubbed texts\n",
    "df2['Incident_Desc_Scrubbed'] = scrubbed_texts\n",
    "df2['Incident_Desc_Scrubbed'] = df2['Incident_Desc_Scrubbed'].apply(revert_patterns)\n",
    "\n",
    "#Rename 'Incident_Desc_Scrubbed' column\n",
    "df2.drop(columns=['Event Description'], inplace=True)\n",
    "df2.rename({'Incident_Desc_Scrubbed': 'Event Description'}, axis=1, inplace=True)\n",
    "df2.insert(11, 'Event Description', df2.pop('Event Description'))\n",
    "\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df2.to_csv('Scrubbed_S&S_August2024_test.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f09e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
